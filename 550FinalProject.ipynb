{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 550 Final Project \n",
    "\n",
    "Luka Petrovic UCID: <br>\n",
    "Logan Boras UCID: <br> \n",
    "Kenny Jeon UCID: 30068677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up environment and Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/bitnami/python/lib/python3.8/site-packages (23.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: findspark in /opt/bitnami/python/lib/python3.8/site-packages (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /opt/bitnami/python/lib/python3.8/site-packages (1.24.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# uncomment the following if you do not have spark installed in your project\n",
    "#!wget --no-check-certificate https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
    "#!tar -xvf spark-3.3.3-bin-hadoop3.tgz\n",
    "!pip install findspark\n",
    "!pip install numpy\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/training/spark-3.3.3-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Seng550Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create directory for input data to live ###\n",
    "\n",
    "Note that the deerfoot.csv file or whatever input data you are using must be within the data folder then proceed to store it on the hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "put: `/inputs/data/deerfoot5Lines.csv': File exists\n",
      "put: `/inputs/data/deerfoot.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!hadoop fs -mkdir -p /inputs\n",
    "!hadoop fs -put data /inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grab the specific data we want from the data set ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21/09/2013,Saturday,34,34,34,34,35,34,35,36,38,36,36,35,35,35,35,35,36,34,34,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2']\n"
     ]
    }
   ],
   "source": [
    "file = '/inputs/data/deerfoot.csv'\n",
    "\n",
    "deerfootRDD = sc.textFile(file, 8)\n",
    "print(deerfootRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Now we want to get the data only for morning and afternoon rush hour, 8am and 4pm will suffice ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saturday', '35', '35')]\n"
     ]
    }
   ],
   "source": [
    "def getRushHours(RDD):\n",
    "\n",
    "    rushHours = RDD.split(',')\n",
    "    return (rushHours[1],rushHours[6],rushHours[13])\n",
    "\n",
    "rushHoursRDD = deerfootRDD.map(getRushHours)\n",
    "print(rushHoursRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Rush hour generally doesn't happen on weekends, so we need to filter out Saturdays and Sundays ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Monday', '45', '40'), ('Tuesday', '52', '40'), ('Wednesday', '39', '40'), ('Thursday', '49', '56'), ('Friday', '36', '43')]\n"
     ]
    }
   ],
   "source": [
    "# This function was taken from the Spark Basics Tutorial\n",
    "\n",
    "def removeWeekends(deerfootRDDRecord):\n",
    "    if deerfootRDDRecord[0]==\"Saturday\" or deerfootRDDRecord[0]==\"Sunday\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "weekdayRushHoursRDD = rushHoursRDD.filter(removeWeekends)\n",
    "print(weekdayRushHoursRDD.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute Statistics and Averages\n",
    "\n",
    "For our analysis, we want to get both the weekly AM and PM averages, as well as the overall average. For a day to be considered \"bad\", the mean of the AM and PM commute times should be higher than the overall average commute time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Get day counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "totalCount = weekdayRushHoursRDD.count()\n",
    "print(totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Get overall rush hour average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.795138888888886\n"
     ]
    }
   ],
   "source": [
    "totalRushHourSum = sc.accumulator(0) # total average\n",
    "\n",
    "\n",
    "def sum(item):\n",
    "    global totalRushHourSum\n",
    "    totalRushHourSum += int(item[1])\n",
    "    totalRushHourSum += int(item[2])\n",
    "    \n",
    "weekdayRushHoursRDD.foreach(sum)\n",
    "totalRushHourAverage = totalRushHourSum.value/(totalCount*2)\n",
    "print(totalRushHourAverage)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce down am and pm commute times into a daily average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Monday', 42.5), ('Tuesday', 46.0), ('Wednesday', 39.5), ('Thursday', 52.5), ('Friday', 39.5)]\n"
     ]
    }
   ],
   "source": [
    "weekdayAverageRushHoursRDD = weekdayRushHoursRDD.map(lambda x: (x[0], (int(x[1])+int(x[2]))/2))\n",
    "print(weekdayAverageRushHoursRDD.take(5))\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Prepare data for model\n",
    "\n",
    "We want to add a \"good\" or \"bad\" label to each field. In order to do this, we will compare the daily average commute time with the total average. If it's higher, its a bad day. If its lower, consider it a good day. We will use the integer 1 as good and 0 as bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a - Add labels to data\n",
    "\n",
    "We want to add a \"good\" or \"bad\" label to each field. In order to do this, we will compare the daily average commute time with the total average. If it's higher, its a bad day. If its lower, consider it a good day. We will use the integer 1 as good and 0 as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Monday', 42.5), (0, 'Tuesday', 46.0)]\n"
     ]
    }
   ],
   "source": [
    "def addLabels(item):\n",
    "    goodOrBadLabel = 1\n",
    "    if(item[1] > totalRushHourAverage):\n",
    "        goodOrBadLabel = 0\n",
    "        \n",
    "    return (goodOrBadLabel,item[0], item[1])\n",
    "\n",
    "\n",
    "labeledRDD = weekdayAverageRushHoursRDD.map(addLabels)\n",
    "print(labeledRDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b - Convert days of the week to integers\n",
    "We want to use the weekday as a feature, but you can't use a string as a feature. This will convert the weekday into a 1-5 value, 1 being monday and 5 being friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 42.5), (0, 2, 46.0)]\n"
     ]
    }
   ],
   "source": [
    "def stringToInt(item):\n",
    "    weekdayInt = 0;\n",
    "    weekday = item[1]\n",
    "    \n",
    "    if weekday == 'Monday':\n",
    "        weekdayInt = 1\n",
    "    elif weekday == 'Tuesday':\n",
    "        weekdayInt = 2\n",
    "    elif weekday == 'Wednesday':\n",
    "        weekdayInt = 3\n",
    "    elif weekday == 'Thursday':\n",
    "        weekdayInt = 4\n",
    "    elif weekday == 'Friday':\n",
    "        weekdayInt = 5\n",
    "    else:\n",
    "        weekdayInt = 1  # Bad data, default to Monday\n",
    "      \n",
    "    return (item[0], weekdayInt, item[2])    \n",
    "        \n",
    "labeledConvertedRDD = labeledRDD.map(stringToInt)\n",
    "print(labeledConvertedRDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5c - Convert data to LabeledPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [1.0,42.5])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(item):\n",
    "    labeledPoint = LabeledPoint(item[0], item[1:])\n",
    "    return labeledPoint\n",
    "\n",
    "finalRDD = labeledConvertedRDD.map(parsePoint)\n",
    "print(finalRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Set up logistic regression model for prediction\n",
    "\n",
    "Now that we have our inputs sorted, we can start to make our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6a - Make and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "trainingData, testingData = finalRDD.randomSplit([.8, .2],21)\n",
    "print(trainingData.count())\n",
    "print(testingData.count())\n",
    "\n",
    "# trainingData.cache()\n",
    "# testingData.cache()\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evalute Model\n",
    "\n",
    "Now that we have our model built and trained, we will evaluate it with test data. We will evalutae it based on the area under the precision-recall curve and the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/training/spark-3.3.3-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/training/spark-3.3.3-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/training/spark-3.3.3-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/training/spark-3.3.3-bin-hadoop3/python/pyspark/context.py:561: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "predictions = test.map(lambda x: (float(model.predict(x.features)), x.label))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictions)\n",
    "\n",
    "print(\"Area under precision-recall curve = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "print(\"Area under ROC curve = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
